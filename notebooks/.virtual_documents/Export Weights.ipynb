








import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.models import load_model, Model, Sequential
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate
from keras.layers.core import Dropout, Lambda
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers import Input, Dropout, Activation, Conv2D, MaxPooling2D, UpSampling2D, Lambda, BatchNormalization
from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from keras import callbacks, initializers, layers, models, optimizers
from keras import backend as K

from pathlib import Path
import numpy as np





import sys
%load_ext autoreload

%autoreload 2

repo_path = Path("..")

sys.path.append("..")
import visualization_utils





# MODEL
model_name = "c-ResUnet.h5"
# model_name = "c-ResUnet_noWM.h5"
model_path = "{}/model_results/{}".format(repo_path, model_name)


def mean_iou(y_true, y_pred):
    prec = []
    for t in np.arange(0.2, 0.8, 0.05):
        y_pred_ = tf.to_int32(y_pred > t)
        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)
        K.get_session().run(tf.local_variables_initializer())
        with tf.control_dependencies([up_opt]):
            score = tf.identity(score)
        prec.append(score)
    return K.mean(K.stack(prec), axis=0)

# dice loss


def dice_coef(y_true, y_pred):
    """Generate the 'Dice' coefficient for the provided prediction.
    Args:
        y_true: The expected/desired output mask.
        y_pred: The actual/predicted mask.
    Returns:
        The Dice coefficient between the expected and actual outputs. Values
        closer to 1 are considered 'better'.
    """
    smooth = 1.
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)


def dice_coef_loss(y_true, y_pred):
    """Model loss function using the 'Dice' coefficient.
    Args:
        y_true: The expected/desired output mask.
        y_pred: The actual/predicted mask.
    Returns:
        The corresponding loss, related to the dice coefficient between the expected
        and actual outputs. Values closer to 0 are considered 'better'.
    """
    return -dice_coef(y_true, y_pred)


def create_weighted_binary_crossentropy(zero_weight, one_weight):

    def weighted_binary_crossentropy(y_true, y_pred):

        b_ce = K.binary_crossentropy(y_true, y_pred)

        # Apply the weights
        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight
        weighted_b_ce = weight_vector * b_ce

        # Return the mean error
        return K.mean(weighted_b_ce)

    return weighted_binary_crossentropy





WeightedLoss = create_weighted_binary_crossentropy(1, 1.5)

model = load_model(model_path, custom_objects={'mean_iou': mean_iou, 'dice_coef': dice_coef,
                                               'weighted_binary_crossentropy': WeightedLoss}, compile=False)


model.summary()





def get_named_weights(model, shapes=False):
    """Return PyTorch's `state_dict`-like dictionary of model weights."""
    from collections import OrderedDict
    names = [weight.name for layer in model.layers for weight in layer.weights]
    weights = model.get_weights()

    state_dict = OrderedDict()
    state_dict_shapes = OrderedDict()
    for name, weight in zip(names, weights):
        state_dict[name] = weight
        if shapes:
            state_dict_shapes[name] = weight.shape
    return state_dict, state_dict_shapes


state_dict, state_dict_shapes = get_named_weights(model, True)


# w = state_dict['conv2d_1/kernel:0']
# for el in w.flatten():
#     print(repr(el))


idx = 0
for k,v in state_dict_shapes.items():
    print(k, v)
    idx+=1
    if idx==4: break





np.set_printoptions(precision=20)

# w_name = 'conv2d_1/kernel:0'
w_name = 'batch_normalization_1'  # /beta:0'
# w_name = 'conv2d_2'

if 'conv' in w_name:
    print('weight:\n', state_dict[f"{w_name}/kernel:0"],
          '\nbias:\n', state_dict[f"{w_name}/bias:0"])

elif 'batch' in w_name:
    print('weight:\n', state_dict[f"{w_name}/gamma:0"],
          '\nbias:\n', state_dict[f"{w_name}/beta:0"],
          '\nmean:\n', state_dict[f"{w_name}/moving_mean:0"],
          '\nvariance:\n', state_dict[f"{w_name}/moving_variance:0"])
# state_dict[w_name]





import pickle


def save_state_dict(d, path):
    print('Saving at:', path)
    with open(path, 'wb') as f:
        pickle.dump(d, f)


def load_state_dict(path):
    with open(path, 'rb') as f:
        d = pickle.load(f)
        return d


save_folder = f'{repo_path}/model_results/'

# state_dict
outpath = save_folder + f"{model_name.split('.')[0]}_state_dict.pkl"
save_state_dict(state_dict, outpath)
state_dict1 = load_state_dict(outpath)

# state_dict_shapes
outpath = save_folder + f"{model_name.split('.')[0]}_state_dict_shapes.pkl"
save_state_dict(state_dict_shapes, outpath)
state_dict_shapes1 = load_state_dict(outpath)

idx = 0
for old_w, old_s, new_w, new_s in zip(state_dict.keys(), state_dict_shapes.values(), state_dict1.keys(), state_dict_shapes1.values()):
    print('saved dict:\n', 'layer:\t', old_w, '\tshape:\t', old_s)
    print('loaded dict:\n', 'layer:\t', new_w, '\tshape:\t', new_s)
    print('comparison:\nold:', state_dict[old_w], '\nnew:', state_dict1[new_w])
    print('\n\n')
    idx += 1
    if idx > 4:
        break


outpath



