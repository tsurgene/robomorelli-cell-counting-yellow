








import tensorflow as tf
from keras.preprocessing.image import ImageDataGenerator
from keras.models import load_model, Model, Sequential
from keras.layers.pooling import MaxPooling2D
from keras.layers.merge import concatenate
from keras.layers.core import Dropout, Lambda
from keras.layers.convolutional import Conv2D, Conv2DTranspose
from keras.layers import Input, Dropout, Activation, Conv2D, MaxPooling2D, UpSampling2D, Lambda, BatchNormalization
from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint
from keras import callbacks, initializers, layers, models, optimizers
from keras import backend as K

from pathlib import Path





%load_ext autoreload

%autoreload 2

repo_path = Path("..")

import sys
sys.path.append("..")

import visualization_utils





# set paths where to read images and masks
# SAMPLE_IMG_PATH = repo_path / "DATASET/test_tr_opt/sample_valid/images"
# SAMPLE_MASKS_PATH = repo_path / "DATASET/test_tr_opt/sample_valid/masks"
SAMPLE_IMG_PATH = repo_path / "DATASET/test/all_images/images"
SAMPLE_MASKS_PATH = repo_path / "DATASET/test/all_masks/masks"
    
### MODEL
model_name = "c-ResUnet.h5"
model_path = "{}/model_results/{}".format(repo_path, model_name)


def mean_iou(y_true, y_pred):
    prec = []
    for t in np.arange(0.2, 0.8, 0.05):
        y_pred_ = tf.to_int32(y_pred > t)
        score, up_opt = tf.metrics.mean_iou(y_true, y_pred_, 2)
        K.get_session().run(tf.local_variables_initializer())
        with tf.control_dependencies([up_opt]):
            score = tf.identity(score)
        prec.append(score)
    return K.mean(K.stack(prec), axis=0)

# dice loss


def dice_coef(y_true, y_pred):
    """Generate the 'Dice' coefficient for the provided prediction.
    Args:
        y_true: The expected/desired output mask.
        y_pred: The actual/predicted mask.
    Returns:
        The Dice coefficient between the expected and actual outputs. Values
        closer to 1 are considered 'better'.
    """
    smooth = 1.
    y_true_f = K.flatten(y_true)
    y_pred_f = K.flatten(y_pred)
    intersection = K.sum(y_true_f * y_pred_f)
    return (2. * intersection + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)


def dice_coef_loss(y_true, y_pred):
    """Model loss function using the 'Dice' coefficient.
    Args:
        y_true: The expected/desired output mask.
        y_pred: The actual/predicted mask.
    Returns:
        The corresponding loss, related to the dice coefficient between the expected
        and actual outputs. Values closer to 0 are considered 'better'.
    """
    return -dice_coef(y_true, y_pred)



def create_weighted_binary_crossentropy(zero_weight, one_weight):

    def weighted_binary_crossentropy(y_true, y_pred):

        b_ce = K.binary_crossentropy(y_true, y_pred)

        # Apply the weights
        weight_vector = y_true * one_weight + (1. - y_true) * zero_weight
        weighted_b_ce = weight_vector * b_ce

        # Return the mean error
        return K.mean(weighted_b_ce)

    return weighted_binary_crossentropy


WeightedLoss = create_weighted_binary_crossentropy(1, 1.5)

model = load_model(model_path, custom_objects={'mean_iou': mean_iou, 'dice_coef': dice_coef, 
                                                    'weighted_binary_crossentropy': WeightedLoss}, compile=False)   





visualization_utils.plot_postprocessing_effect(model,SAMPLE_IMG_PATH, threshold=0.875, example_only=True)#, suptitle=False, head=3)


visualization_utils.plot_predicted_heatmaps(model,SAMPLE_IMG_PATH, SAMPLE_MASKS_PATH,  example_only=True)#, suptitle=False, head=3)











visualization_utils.plot_predicted_mask(model,SAMPLE_IMG_PATH, SAMPLE_MASKS_PATH, threshold=0.875, post_processing=False)











visualization_utils.plot_predicted_mask(model,SAMPLE_IMG_PATH, SAMPLE_MASKS_PATH, threshold=0.875, post_processing=True, suptitle=False)



